---
draft: false
title: "Ethics & Morals"
snippet: "Ornare cum cursus laoreet sagittis nunc fusce posuere per euismod dis vehicula a, semper fames lacus maecenas dictumst pulvinar neque enim non potenti. Torquent hac sociosqu eleifend potenti."
image: {
    src: "",
    alt: "data structures & algorithms"
}
publishDate: "2090-04-16 16:39"
category: "Deep Dive"
author: "Junead Khan"
tags: [ethics, morals, trolley problem]
---

Autonomous vehicles (AVs) have the
potential to improve mobility, reduce traffic congestion, save fuel, and
prevent human errors that cause accidents.However, AVs also raise
ethical and moral questions that need to be addressed before they become
widely adopted.

One of the main ethical challenges of AVs is how they
should behave in unavoidable collision scenarios, also known as moral
dilemmas. These are situations where an AV has to choose between two or
more harmful outcomes, such as hitting a pedestrian or a cyclist,
swerving into another lane or a wall, or sacrificing its own passengers
or other drivers. 

![Trolley Problem](https://edge.mcsw.net/mcsweeneys/zculq7fmfbt0pdj1g3wq7mu4u86a) Image Source: Business Thoughts


### How should an AV decide who to save and who to harm?
 What criteria should it use to make such decisions? Who should be
responsible for programming these criteria and ensuring their fairness
and transparency? A core issue today is how AVs should make decisions in situations that involve unavoidable harm, such as running over pedestrians or sacrificing themselves and their passengers to save them. 

    The trolley problem is a classic thought experiment in moral philosophy that asks
    whether it is better to kill one person or five people by pulling a lever that
    switches the tracks of a runaway trolley.

A recent paper by Bonnefon et al. (2016) titled "The social dilemma of autonomous vehicles" explores this issue using a series of surveys with participants from different countries and backgrounds. The paper found that participants generally approved of utilitarian AVs, that is, AVs that sacrifice their passengers for the greater good, and would like others to buy them. However, they would themselves prefer to ride in AVs that protect their passengers at all costs. They would also disapprove of enforcing utilitarian regulations for AVs and would be less willing to buy such an AV.

The paper argues that this creates a social dilemma, where people act in their own self-interest rather than the public good, which may paradoxically increase casualties by postponing the adoption of a safer technology. The paper suggests that this dilemma can be resolved by designing AVs that are transparent about their decision-making algorithms, that allow users to express their preferences and values, and that balance the trade-offs between safety, efficiency, and ethics.

The paper raises some important questions about the ethics and morals of autonomous vehicles, such as:

- Who should be responsible for programming the moral algorithms of AVs?
- How should AVs balance the rights and interests of different stakeholders, such as pedestrians, passengers, manufacturers, regulators, and society at large?
- How should AVs communicate their decisions and actions to other road users and authorities?
- How should AVs respect the cultural and legal norms of different regions and countries?
- How should AVs cope with uncertainty and ambiguity in complex situations?

These questions are not easy to answer, but they need to be addressed before AVs can be fully integrated into our society. As Bonnefon et al. (2016) conclude: "The sooner we begin this discussion, the more prepared we will be for a future with AVs."

### Can you program ethics into a self-driving car?

![Self-Driving Car Accident](https://wp.technologyreview.com/wp-content/uploads/2018/10/m.i.tsmartcarchoicescolo01-9.jpg) Image Source: MIT Technology Review

It is all well and good to discuss what ethics and morals should guide the behavior of autonomous vehicles, but how do we actually implement these principles in practice?

One paper that explores this question is "Can You Program Ethics Into a Self-Driving Car?" by Noah J. Goodall, published in IEEE Spectrum in 2016. The paper discusses the challenges and implications of creating ethical rules for automated vehicles.

The paper argues that there is no simple or universal answer to the question of programming ethics into a self-driving car. Instead, it suggests that engineers, policymakers, and society at large need to engage in a dialogue and a process of experimentation and learning to find the best solutions for different contexts and scenarios. The paper also warns that ethical decisions made by programmers may have unintended consequences or be subject to manipulation or hacking by malicious actors.

The paper offers some insights and recommendations for addressing the ethical challenges of self-driving cars, such as:

- Developing a framework of ethical principles and values that can guide the design and regulation of self-driving cars, based on existing theories of ethics and moral psychology.
- Creating transparent and accountable mechanisms for monitoring and evaluating the performance and behavior of self-driving cars, as well as for reporting and resolving any incidents or disputes involving them.
- Incorporating human feedback and input into the decision-making process of self-driving cars, either through direct communication or through learning from human drivers' behavior and preferences.
- Balancing the trade-offs between safety, efficiency, convenience, and fairness when designing and deploying self-driving cars, taking into account the needs and expectations of various stakeholders and users.
- Recognizing and respecting the diversity and complexity of human values and cultures that may influence how people view and interact with self-driving cars, and avoiding imposing a single or dominant perspective on them.

The paper concludes that programming ethics into a self-driving car is not only a technical challenge, but also a social and political one. It calls for a collaborative and adaptive approach that involves multiple disciplines and perspectives, as well as public engagement and education. It also acknowledges that there may be no perfect or final solution to this problem, but rather an ongoing process of improvement and refinement.

### Who is responsible for accidents involving autonomous vehicles?

![Self-Driving Car Accident](https://ewscripps.brightspotcdn.com/dims4/default/768b0af/2147483647/strip/true/crop/640x360+0+60/resize/1280x720!/quality/90/?url=https%3A%2F%2Fmediaassets.abc15.com%2Fphoto%2F2018%2F05%2F04%2Fwaymo_1525481747333_85910649_ver1.0_640_480.jpg) Image Source: ABC15 Arizona


Crashes are inevitable, even for autonomous vehicles. In such cases, who should be held accountable for the damage and harm caused by the vehicle? Is it the manufacturer, who designed and produced the vehicle? Is it the user, who chose to use the vehicle and may have some degree of control over it? Or is it the vehicle itself, which may have some level of autonomy and decision-making capability?

This question is not only a legal one, but also a moral one. A 2014 paper by Alexander Hevelke and Julian Nida-RÃ¼melin titled "Responsibility for Crashes of Autonomous Vehicles: An Ethical Analysis", explores this question from different perspectives and proposes some possible solutions.

The paper begins by examining the option of holding the manufacturers responsible for any crash caused by the vehicle. This option seems to be the most obvious one, as it follows the principle of product liability, which states that the producer of a defective product should bear the costs of any harm caused by it. However, the paper argues that this option may have some drawbacks. First, it may discourage the manufacturers from developing and improving autonomous vehicles, as they would face a high risk of litigation and compensation. Second, it may create a moral hazard problem, where the users of autonomous vehicles would have no incentive to pay attention to the road or intervene when necessary, as they would not be liable for any accident.

The paper then considers the option of holding the user responsible for any crash caused by the vehicle. This option seems to be more fair and reasonable, as it follows the principle of strict liability, which states that anyone who engages in a risky activity should bear the costs of any harm caused by it. However, this option also has some challenges. First, it depends on whether the user has any way or duty of influencing or intervening in the vehicle's behavior. If the user has no way or duty of doing so, then it would be unjust to hold him or her responsible for something he or she could not prevent or control. If the user has some way or duty of doing so, then it would depend on whether he or she had a realistic chance to do so in a given situation. For example, if an autonomous vehicle suddenly encounters a pedestrian crossing the road illegally, would the user have enough time and information to react and override the vehicle's decision? Second, it may create some practical difficulties in determining whether an accident was caused by the vehicle or by the user's intervention or negligence. For example, if an autonomous vehicle swerves to avoid a collision with another car but hits a tree instead.

The last option discussed is a system in which a person using an autonomous vehicle has no duty of interfering, but is still held financially, but not criminally, responsible for possible accidents. The discussion highlights the difficult nature of the problem and the need for further research and debate.

### To sum up...
In conclusion, autonomous vehicles pose significant ethical and moral challenges that need to be addressed by policymakers, manufacturers, and users. While they have the potential to reduce traffic accidents, emissions, and congestion, they also raise questions about responsibility, accountability, and justice.
